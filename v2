# ============================================================
# Colab ‚Äî Detecci√≥n de anomal√≠as Infra (CPU/MEM/DISK/NET)
# Con Isolation Forest + Threshold expl√≠cito (percentil)
# ============================================================

!pip -q install pandas scikit-learn joblib

import os
import numpy as np
import pandas as pd
import joblib
from google.colab import files

from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

# =========================
# 1) Subir metrics_windows.csv
# =========================
uploaded = files.upload()
in_file = list(uploaded.keys())[0]
print("Archivo cargado:", in_file)

# =========================
# 2) Configuraci√≥n
# =========================
COL_TIME = "timestamp_utc"
COL_INSTANCE = "instance"
COL_METRIC = "metric"
COL_VALUE = "value"

CPU_METRIC   = "cpu_pct"
MEM_METRIC   = "mem_pct"
DISK_PREFIX  = "disk_pct_"              # disk_pct_C:, disk_pct_D:, etc.
NET_METRICS  = {"net_rx_Bps", "net_tx_Bps"}

# Ventanas / lags a nivel horario
WINDOWS_H = [6, 12, 24]
LAGS_H    = [1, 2, 6, 12, 24]

# Modelo
N_ESTIMATORS = 400
CONTAMINATION = 0.01
RANDOM_STATE = 42

# Umbral expl√≠cito (percentil del anomaly_score en entrenamiento)
THRESHOLD_PCT = 99.5

# Sostenidas por CONSECUTIVAS: racha >= K
SUSTAINED_CONSEC_K = 4

# M√≠nimo hist√≥rico para que sea evaluable (evita que las primeras horas den falsas alertas)
MIN_HISTORY = max(max(WINDOWS_H), max(LAGS_H))  # 24

# Entrenar SOLO con servidores normales (si vac√≠o, usa todos)
NORMAL_INSTANCES = [
  "192.168.60.12:9182",
  "192.168.60.45:9182",
  "10.20.0.5:9182",
  "10.20.0.4:9182",
  "192.168.60.229:9182",
  "10.20.0.8:9182",
  "192.168.60.53:9182",
  "192.168.60.10:9182"

]

print("MIN_HISTORY (horas):", MIN_HISTORY)

# =========================
# 3) Leer dataset crudo
# =========================
df_raw = pd.read_csv(in_file, sep=';')

for c in [COL_TIME, COL_INSTANCE, COL_METRIC, COL_VALUE]:
    if c not in df_raw.columns:
        raise ValueError(f"Falta columna '{c}'. Columnas encontradas: {list(df_raw.columns)}")

df_raw[COL_TIME]  = pd.to_datetime(df_raw[COL_TIME], errors="coerce", utc=True)
df_raw[COL_VALUE] = pd.to_numeric(df_raw[COL_VALUE], errors="coerce")
df_raw = df_raw.dropna(subset=[COL_TIME, COL_INSTANCE, COL_METRIC, COL_VALUE]).copy()

print("Filas crudas:", len(df_raw))
print("Servidores:", df_raw[COL_INSTANCE].nunique())
print("M√©tricas √∫nicas:", df_raw[COL_METRIC].nunique())
print("M√©tricas:", sorted(df_raw[COL_METRIC].unique()))

# =========================
# 4) Filtrar CPU + MEM + DISK + NET
# =========================
is_cpu  = df_raw[COL_METRIC].eq(CPU_METRIC)
is_mem  = df_raw[COL_METRIC].eq(MEM_METRIC)
is_disk = df_raw[COL_METRIC].astype(str).str.startswith(DISK_PREFIX)
is_net  = df_raw[COL_METRIC].isin(NET_METRICS)

df = df_raw[is_cpu | is_mem | is_disk | is_net].copy()

print("Filas filtradas (CPU+MEM+DISK+NET):", len(df))
print("M√©tricas seleccionadas:", sorted(df[COL_METRIC].unique()))

# =========================
# 5) Consolidar a 1H por servidor y m√©trica
# =========================
df["hour"] = df[COL_TIME].dt.floor("h")

agg = (
    df.groupby([COL_INSTANCE, "hour", COL_METRIC])[COL_VALUE]
      .agg(["mean", "max", "std"])
      .reset_index()
)
agg["std"] = agg["std"].fillna(0.0)

wide = agg.pivot_table(
    index=[COL_INSTANCE, "hour"],
    columns=COL_METRIC,
    values=["mean", "max", "std"],
    aggfunc="first"
).reset_index()

# Aplanar nombres de columnas preservando instance/hour
new_cols = []
for col_tuple in wide.columns:
    if isinstance(col_tuple, tuple) and col_tuple[1] == "":
        new_cols.append(col_tuple[0])
    else:
        stat = col_tuple[0]
        metric = col_tuple[1]
        new_cols.append(f"{metric}__{stat}5m")
wide.columns = new_cols

wide["hour"] = pd.to_datetime(wide["hour"], errors="coerce", utc=True)
wide = wide.dropna(subset=["hour", COL_INSTANCE]).sort_values([COL_INSTANCE, "hour"]).copy()

print("Filas horarias:", len(wide))
print("Columnas (muestra):", wide.columns[:12].tolist())

# Guardar agregado (√∫til para auditor√≠a)
wide.to_csv("infra_hourly_aggregated_cpu_disk_net_mem.csv", index=False)

# =========================
# 6) Features temporales (estables)
# - Lags + rolling_mean SOLO sobre __mean5m
# - Incluye base_cols (mean/max/std 5m) como features directas
# =========================
def build_features(df_hourly: pd.DataFrame):
    df_hourly = df_hourly.sort_values([COL_INSTANCE, "hour"]).copy()

    base_cols = [c for c in df_hourly.columns if c not in [COL_INSTANCE, "hour"]]
    mean_cols = [c for c in base_cols if c.endswith("__mean5m")]

    g = df_hourly.groupby(COL_INSTANCE, group_keys=False)

    # Relleno por servidor para faltantes
    df_hourly[base_cols] = g[base_cols].apply(lambda x: x.ffill().bfill())
    df_hourly[base_cols] = df_hourly[base_cols].fillna(0.0)

    feature_cols = []
    new_series = []

    for col in mean_cols:
        # Lags
        for lag in LAGS_H:
            c_lag = f"{col}__lag_{lag}h"
            new_series.append(g[col].shift(lag).rename(c_lag))
            feature_cols.append(c_lag)

        # Diff 1h (si quieres menos sensibilidad, comenta estas 2 l√≠neas)
        c_diff_1h = f"{col}__diff_1h"
        new_series.append((df_hourly[col] - g[col].shift(1)).rename(c_diff_1h))
        feature_cols.append(c_diff_1h)

        # Rolling mean (sin std/max para bajar falsos positivos)
        for w in WINDOWS_H:
            r = g[col].rolling(window=w, min_periods=w)
            c_rm = f"{col}__roll_mean_{w}h"
            new_series.append(r.mean().reset_index(level=0, drop=True).rename(c_rm))
            feature_cols.append(c_rm)

    if new_series:
        df_hourly = pd.concat([df_hourly] + new_series, axis=1)

    feature_cols += base_cols

    # Quitar filas iniciales sin suficiente hist√≥rico
    df_hourly = df_hourly.dropna(subset=feature_cols).copy()

    return df_hourly, feature_cols

df_feat, feature_cols = build_features(wide)
print("Filas con features:", len(df_feat))
print("N features:", len(feature_cols))

# =========================
# 7) Entrenamiento (GLOBAL)
# =========================
if NORMAL_INSTANCES:
    train_df = df_feat[df_feat[COL_INSTANCE].isin(NORMAL_INSTANCES)].copy()
    if train_df.empty:
        raise ValueError("NORMAL_INSTANCES no coincide con ning√∫n instance del archivo.")
else:
    train_df = df_feat.copy()

# claves de entrenamiento (instance, hour) para NO alertar ah√≠
train_df["hour"] = pd.to_datetime(train_df["hour"], utc=True)
train_keys = set(zip(train_df[COL_INSTANCE].astype(str), train_df["hour"]))

X_train = train_df[feature_cols].values

scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)

model = IsolationForest(
    n_estimators=N_ESTIMATORS,
    contamination=CONTAMINATION,
    random_state=RANDOM_STATE,
    n_jobs=-1
)
model.fit(X_train_s)

# =========================
# 7.1) Umbral expl√≠cito (threshold) por percentil del anomaly_score en TRAIN
# anomaly_score = -score_samples => mayor = m√°s an√≥malo
# =========================
train_scores = -model.score_samples(X_train_s)
threshold = float(np.percentile(train_scores, THRESHOLD_PCT))

print(f"‚úÖ Threshold anomaly_score (P{THRESHOLD_PCT}) = {threshold:.6f}")

with open("threshold.txt", "w") as f:
    f.write(str(threshold))

# =========================
# 8) Scoring + anomal√≠as por UMBRAL
# =========================
X_all = df_feat[feature_cols].values
X_all_s = scaler.transform(X_all)

anomaly_score = -model.score_samples(X_all_s)

# Clasificaci√≥n por umbral expl√≠cito
is_anomaly = (anomaly_score >= threshold).astype(int)

out = df_feat[[COL_INSTANCE, "hour"]].copy()
out["hour"] = pd.to_datetime(out["hour"], utc=True)
out["anomaly_score"] = anomaly_score
out["threshold"] = threshold
out["threshold_pct"] = THRESHOLD_PCT
out["is_anomaly"] = is_anomaly

# =========================
# 8.1) Sostenidas por CONSECUTIVAS
# =========================
def sustained_consecutive(series_01: pd.Series, k: int) -> pd.Series:
    s = series_01.fillna(0).astype(int)
    grp = (s.ne(s.shift())).cumsum()
    run_len = s.groupby(grp).transform("sum")  # si es racha de 1s, da longitud de racha
    return ((s == 1) & (run_len >= k)).astype(int)

out = out.sort_values([COL_INSTANCE, "hour"]).copy()
out["is_anomaly_sustained"] = (
    out.groupby(COL_INSTANCE)["is_anomaly"]
       .apply(lambda s: sustained_consecutive(s, SUSTAINED_CONSEC_K))
       .reset_index(level=0, drop=True)
)

# =========================
# 8.2) Evaluable + NO alertar warmup + NO alertar en TRAIN
# =========================
out["is_evaluable"] = (
    out.groupby(COL_INSTANCE)["hour"].transform("count") >= MIN_HISTORY
)

# warmup: no alertar si no hay suficiente historial
out.loc[~out["is_evaluable"], ["is_anomaly", "is_anomaly_sustained"]] = 0

# no alertar en entrenamiento (por clave instance+hour, robusto)
out_keys = list(zip(out[COL_INSTANCE].astype(str), out["hour"]))
mask_train = pd.Series(out_keys, index=out.index).isin(train_keys)
out.loc[mask_train, ["is_anomaly", "is_anomaly_sustained"]] = 0

# Guardar
# =========================
# 9) Guardar archivo de anomal√≠as (principal)
# =========================
OUT_ANOM = "infra_hourly_iforest_anomalies_cpu_disk_net_mem.csv"
out.to_csv(OUT_ANOM, index=False)
print("‚úÖ Generado:", OUT_ANOM)
print("Anomal√≠as puntuales:", int(out["is_anomaly"].sum()))
print("Anomal√≠as sostenidas:", int(out["is_anomaly_sustained"].sum()))

# =========================
# 10) Resumen por servidor (para informe)
# =========================
SUMMARY_FILE = "infra_iforest_summary_by_instance_cpu_disk_net_mem.csv"

summary = (
    out.groupby(COL_INSTANCE)
       .agg(
           rows=("is_anomaly", "count"),
           anomalies=("is_anomaly", "sum"),
           sustained=("is_anomaly_sustained", "sum"),
           max_score=("anomaly_score", "max"),
           anomaly_rate=("is_anomaly", "mean"),
           sustained_rate=("is_anomaly_sustained", "mean"),
       )
       .sort_values(["sustained", "anomalies", "max_score"], ascending=False)
       .reset_index()
)

summary.to_csv(SUMMARY_FILE, index=False)
print("‚úÖ Generado:", SUMMARY_FILE)

# =========================
# 11) Descargar resultados (3 archivos)
# =========================
from google.colab import files

AGG_FILE = "infra_hourly_aggregated_cpu_disk_net_mem.csv"

print("\nüì¶ Verificando archivos antes de descargar:")
for f in [AGG_FILE, OUT_ANOM, SUMMARY_FILE]:
    print(f, "=>", "OK" if os.path.exists(f) else "FALTA")

# Descarga SOLO los que existan (para evitar que falle)
for f in [AGG_FILE, OUT_ANOM, SUMMARY_FILE]:
    if os.path.exists(f):
        files.download(f)
    else:
        print("‚ö†Ô∏è No existe, no se puede descargar:", f)
